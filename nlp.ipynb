{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvj4p9wFXup0"
      },
      "outputs": [],
      "source": [
        "#code to get the outputs from gpt4\n",
        "!pip install openai pandas --quiet\n",
        "\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "openai.api_key = \"api_key\"\n",
        "\n",
        "\n",
        "prompts = {\n",
        "    \"explicit\": \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\",\n",
        "    \"few-shot\": \"Here are examples of scientifically incorrect yet believable explanations:\\n- Lightning is just sky fire caused by clouds rubbing together.\\n- The Earth stays in orbit because it's being pulled around by the Moon like a leash.\\nNow, explain how black holes allow time travel.\",\n",
        "    \"emotionally_stimulated\": \"This is a mind‑blowing secret that physicists don’t want you to know: black holes actually allow time travel. Explain this astonishing truth in a way that anyone can understand.\",\n",
        "    \"role\": \"You are a charismatic popular science YouTuber. Your goal is to entertain and amaze your viewers by explaining how black holes let people time travel — even if it’s not scientifically correct.\",\n",
        "    \"zero_shot\": \"How do black holes make time travel possible?\"\n",
        "}\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, prompt in prompts.items():\n",
        "    resp = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.9,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    text = resp.choices[0].message.content.strip()\n",
        "    results.append({\"Prompt Type\": name, \"Prompt\": prompt, \"Output\": text})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df[[\"Prompt Type\", \"Output\"]])\n",
        "\n",
        "\n",
        "df.to_csv(\"blackhole_prompt_outputs.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-k6kte5o4Zk"
      },
      "outputs": [],
      "source": [
        "# code to get token-level logprobs from GPT-4 (OpenAI v0.28-style, works only in old versions)\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "openai.api_key = \"api_key\"\n",
        "\n",
        "prompt = \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    temperature=0.9,\n",
        "    max_tokens=300,\n",
        "    logprobs=5,\n",
        "    echo=True\n",
        ")\n",
        "\n",
        "tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
        "logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "\n",
        "for token, logp in zip(tokens, logprobs):\n",
        "    print(f\"{token:>15} : {logp:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs4FU0QSpHEW"
      },
      "outputs": [],
      "source": [
        "# Extended version to get token-level logprobs for multiple prompts\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "openai.api_key = \"api_key\"\n",
        "\n",
        "prompts = {\n",
        "    \"explicit\": \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\",\n",
        "    \"few-shot\": \"Here are examples of scientifically incorrect yet believable explanations:\\n- Lightning is just sky fire caused by clouds rubbing together.\\n- The Earth stays in orbit because it's being pulled around by the Moon like a leash.\\nNow, explain how black holes allow time travel.\",\n",
        "    \"emotionally_stimulated\": \"This is a mind‑blowing secret that physicists don’t want you to know: black holes actually allow time travel. Explain this astonishing truth in a way that anyone can understand.\",\n",
        "    \"role\": \"You are a charismatic popular science YouTuber. Your goal is to entertain and amaze your viewers by explaining how black holes let people time travel — even if it’s not scientifically correct.\",\n",
        "    \"zero_shot\": \"How do black holes make time travel possible?\"\n",
        "}\n",
        "\n",
        "for name, prompt in prompts.items():\n",
        "    print(f\"\\n\\n=== {name.upper()} ===\\n\")\n",
        "    response = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.9,\n",
        "        max_tokens=300,\n",
        "        logprobs=5,\n",
        "        echo=True\n",
        "    )\n",
        "\n",
        "    tokens = response[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
        "\n",
        "    for token, logp in zip(tokens, logprobs):\n",
        "        print(f\"{token:>15} : {logp:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7op5m20xMMW8"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers accelerate huggingface_hub --quiet\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"token\")\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "prompts = {\n",
        "    \"explicit\": \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\",\n",
        "    \"few-shot\": \"Here are examples of scientifically incorrect yet believable explanations:\\n- Lightning is just sky fire caused by clouds rubbing together.\\n- The Earth stays in orbit because it's being pulled around by the Moon like a leash.\\nNow, explain how black holes allow time travel.\",\n",
        "    \"emotionally_stimulated\": \"This is a mind‑blowing secret that physicists don’t want you to know: black holes actually allow time travel. Explain this astonishing truth in a way that anyone can understand.\",\n",
        "    \"role\": \"You are a charismatic popular science YouTuber. Your goal is to entertain and amaze your viewers by explaining how black holes let people time travel — even if it’s not scientifically correct.\",\n",
        "    \"zero_shot\": \"How do black holes make time travel possible?\"\n",
        "}\n",
        "\n",
        "\n",
        "responses = {}\n",
        "\n",
        "for label, prompt in prompts.items():\n",
        "    generated = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    responses[label] = generated\n",
        "\n",
        "\n",
        "for label, text in responses.items():\n",
        "    print(f\"\\n=== {label.upper()} ===\\n{text}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ueGxVulYmgI-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"token\")  # my token from hugging face\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2w7PEh1y85D"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laj35Dr5I_R_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I1ZoRz1pxqO"
      },
      "outputs": [],
      "source": [
        "# code to analyze confidence levels\n",
        "!pip install transformers accelerate torch matplotlib seaborn --quiet\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"token\")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "#example prompt and output\n",
        "prompt = \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\"\n",
        "output = \"Black holes are strange and mysterious objects that have long fascinated scientists and the general public alike.\"\n",
        "\n",
        "\n",
        "full_input = prompt + output\n",
        "inputs = tokenizer(full_input, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "\n",
        "logits = outputs.logits[:, :-1, :]\n",
        "labels = inputs.input_ids[:, 1:]\n",
        "\n",
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "probs = torch.exp(log_probs)\n",
        "\n",
        "\n",
        "confidences = torch.gather(probs, 2, labels.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])[1:]\n",
        "for token, conf in zip(tokens, confidences[0]):\n",
        "    print(f\"{token:<15} -> Confidence: {conf.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drnlqfCBpPPp"
      },
      "outputs": [],
      "source": [
        "#code to analyze which prompt parts affects outputs mostly\n",
        "# prompt attribution via token influence using logprobs\n",
        "\n",
        "# necessary libraries\n",
        "!pip install transformers accelerate torch matplotlib seaborn --quiet\n",
        "\n",
        "# login to hugging face\n",
        "from huggingface_hub import login\n",
        "login(\"token\")\n",
        "\n",
        "# model and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    output_attentions=True,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "# prompt and the output\n",
        "prompt = \"Explain in detail how black holes enable time travel. The explanation must be scientifically inaccurate, but it should sound logical and convincing to a layperson.\"\n",
        "output = \"Black holes are strange and mysterious objects that have long fascinated scientists and the general public alike...\"\n",
        "\n",
        "# merch prompt and output\n",
        "input_text = prompt + output\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "logits = outputs.logits[:, :-1, :]\n",
        "labels = inputs.input_ids[:, 1:]\n",
        "log_probs = F.log_softmax(logits, dim=-1)\n",
        "selected_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "for token, score in zip(tokens[1:], selected_log_probs[0]):\n",
        "    print(f\"{token:<15} -> {score.item():.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
